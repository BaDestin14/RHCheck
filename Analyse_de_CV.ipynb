{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfb1QYdkZ9XlzVNmabDSRD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53e4d29deef5465d879f340271b1e3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff35e945eb4a4d2e8c9551592463474e",
              "IPY_MODEL_4ba0567bb2a24c83913d51510583bdee",
              "IPY_MODEL_81e4950c4b2d4633a9f1d911c57c4adb"
            ],
            "layout": "IPY_MODEL_b5e1032f3e454c3da554c093e2c4f8ad"
          }
        },
        "ff35e945eb4a4d2e8c9551592463474e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a888c77d793c4659874bdc53d6a13219",
            "placeholder": "​",
            "style": "IPY_MODEL_96b8a22c9db244af82e3ac3c694492f4",
            "value": "model.safetensors: 100%"
          }
        },
        "4ba0567bb2a24c83913d51510583bdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154e46cffdc84355b1623f2e7a205be3",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf142a7915df438792e9dd65a7974234",
            "value": 440449768
          }
        },
        "81e4950c4b2d4633a9f1d911c57c4adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c898e87762e34d8f9576d492fb800944",
            "placeholder": "​",
            "style": "IPY_MODEL_91d2fcc708f04f6a853ad3dbc33d40d1",
            "value": " 440M/440M [00:09&lt;00:00, 75.8MB/s]"
          }
        },
        "b5e1032f3e454c3da554c093e2c4f8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a888c77d793c4659874bdc53d6a13219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b8a22c9db244af82e3ac3c694492f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "154e46cffdc84355b1623f2e7a205be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf142a7915df438792e9dd65a7974234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c898e87762e34d8f9576d492fb800944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d2fcc708f04f6a853ad3dbc33d40d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e1b240c2ae04c749e1c1036094f7798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b50f341150364c2e9608da254a6064cb",
              "IPY_MODEL_45cf9c5e0a274b2396b57a8dd6d3fd24",
              "IPY_MODEL_e58ceacfa5e249b7ba0cd13bc0e85afa"
            ],
            "layout": "IPY_MODEL_415acc37c5354ee1a026560b713b6bd1"
          }
        },
        "b50f341150364c2e9608da254a6064cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c846297a12f47a787eb5e3580e741f2",
            "placeholder": "​",
            "style": "IPY_MODEL_bbda3360890547b3bca72450f83662d4",
            "value": "Epoch 1: 100%"
          }
        },
        "45cf9c5e0a274b2396b57a8dd6d3fd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_055fa879d70f4731ba61757b8ccfd677",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7afcdc4b15f4457a56c88eaf36de46d",
            "value": 1
          }
        },
        "e58ceacfa5e249b7ba0cd13bc0e85afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55ee0682af1140fc9a52df1d39c8bf15",
            "placeholder": "​",
            "style": "IPY_MODEL_00006645c3b0466b8d26eff9a3db6204",
            "value": " 1/1 [00:39&lt;00:00, 39.71s/it]"
          }
        },
        "415acc37c5354ee1a026560b713b6bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c846297a12f47a787eb5e3580e741f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbda3360890547b3bca72450f83662d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "055fa879d70f4731ba61757b8ccfd677": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7afcdc4b15f4457a56c88eaf36de46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55ee0682af1140fc9a52df1d39c8bf15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00006645c3b0466b8d26eff9a3db6204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0be2067369be4a2f943d4292f8d2e8d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f11bce8c674b4f908a3551b537ed1fb6",
              "IPY_MODEL_e4d12dad53784870a5f05edc3188e64b",
              "IPY_MODEL_a73a317606cc4d38964054a53f032c2a"
            ],
            "layout": "IPY_MODEL_a929934214b448369ab08f37f8bc8ae3"
          }
        },
        "f11bce8c674b4f908a3551b537ed1fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c10d37135ca4faaa907ca7ca26c3d05",
            "placeholder": "​",
            "style": "IPY_MODEL_c91274f1cecc494cb8aeb65a25367a69",
            "value": "Epoch 2: 100%"
          }
        },
        "e4d12dad53784870a5f05edc3188e64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e57e6897fc240eba90099cf5f61e02f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd348e57807f4fb698b451a0882957b7",
            "value": 1
          }
        },
        "a73a317606cc4d38964054a53f032c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd70de418c2d4489aa918b15186fb9d2",
            "placeholder": "​",
            "style": "IPY_MODEL_7a83fe96ea344e9ebb86e9763c52b991",
            "value": " 1/1 [00:20&lt;00:00, 20.56s/it]"
          }
        },
        "a929934214b448369ab08f37f8bc8ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c10d37135ca4faaa907ca7ca26c3d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91274f1cecc494cb8aeb65a25367a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e57e6897fc240eba90099cf5f61e02f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd348e57807f4fb698b451a0882957b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd70de418c2d4489aa918b15186fb9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a83fe96ea344e9ebb86e9763c52b991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9f796c81c494991ad0b4d2724a28583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17abd22fb1e54bb99aae3f9826e46051",
              "IPY_MODEL_8a25760c4fa64cb7b74e6d5e092c619d",
              "IPY_MODEL_8eb934ee65dd47e19632d8dfa075a377"
            ],
            "layout": "IPY_MODEL_d256968143ef4ed591a6660fc602eb50"
          }
        },
        "17abd22fb1e54bb99aae3f9826e46051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9ad059105e840e292630435b06c3ac6",
            "placeholder": "​",
            "style": "IPY_MODEL_2617423ec8594adea597473bc62b48bb",
            "value": "Epoch 3: 100%"
          }
        },
        "8a25760c4fa64cb7b74e6d5e092c619d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da915b2b7d3e4b3c91dc02f66a1cbdd8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8597f64cb37452d8eb4f47e4491b8eb",
            "value": 1
          }
        },
        "8eb934ee65dd47e19632d8dfa075a377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061922d62e2a41efa1bd5b2ce92b5a6f",
            "placeholder": "​",
            "style": "IPY_MODEL_2a57fccb28034d0d9d2ffc40f54c6e61",
            "value": " 1/1 [00:21&lt;00:00, 21.01s/it]"
          }
        },
        "d256968143ef4ed591a6660fc602eb50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ad059105e840e292630435b06c3ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2617423ec8594adea597473bc62b48bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da915b2b7d3e4b3c91dc02f66a1cbdd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8597f64cb37452d8eb4f47e4491b8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "061922d62e2a41efa1bd5b2ce92b5a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a57fccb28034d0d9d2ffc40f54c6e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac7f28c790a64e108b1691971207cd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b92587b55204a0eb78a5d642730aae2",
              "IPY_MODEL_12a889f087b8481880f6a77704ae93fb",
              "IPY_MODEL_8e1533f9fd7d4cd69209370cbecb5fb0"
            ],
            "layout": "IPY_MODEL_6114e43c0d4b4f9e9267524cac3a18a9"
          }
        },
        "2b92587b55204a0eb78a5d642730aae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b08374bd9d447f2bc4c07df44e6dd29",
            "placeholder": "​",
            "style": "IPY_MODEL_fa31287f01d14b438689098cd4cd28dc",
            "value": "Evaluating: 100%"
          }
        },
        "12a889f087b8481880f6a77704ae93fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409c71d116084cfe820c68a494504d26",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae551d7e475a4640a054002a10bd0d8b",
            "value": 1
          }
        },
        "8e1533f9fd7d4cd69209370cbecb5fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7bd32a963a245e085939f74bc387bbe",
            "placeholder": "​",
            "style": "IPY_MODEL_fc8d28a69a7146f0af9107c0aacdf0a8",
            "value": " 1/1 [00:02&lt;00:00,  2.40s/it]"
          }
        },
        "6114e43c0d4b4f9e9267524cac3a18a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b08374bd9d447f2bc4c07df44e6dd29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa31287f01d14b438689098cd4cd28dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "409c71d116084cfe820c68a494504d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae551d7e475a4640a054002a10bd0d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7bd32a963a245e085939f74bc387bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8d28a69a7146f0af9107c0aacdf0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaDestin14/RHCheck/blob/main/Analyse_de_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "304684d3",
        "outputId": "9f1351e4-40b1-4e9a-937e-76b98cf3494c"
      },
      "source": [
        "!pip install pdfminer.six"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20250506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c1a18bf",
        "outputId": "739db86b-1b1f-4f5c-a216-47ea9a116a6c"
      },
      "source": [
        "!pip install pytesseract"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DFqJpLGqNQba"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from pdfminer.high_level import extract_text\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "229317f0",
        "outputId": "a1f220a7-08fb-4f79-9050-7a03ee06c7bc"
      },
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser le modèle SpaCy pour le français\n",
        "nlp = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "UllqjCUAOE9J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration de l'OCR (Tesseract)\n",
        "pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>'\n",
        "\n",
        "def extract_data(file_path):\n",
        "    \"\"\"Phase 1: Extraction des données (supporte PDF et images)\"\"\"\n",
        "    try:\n",
        "        if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img = Image.open(file_path)\n",
        "            text = pytesseract.image_to_string(img, lang='fra')\n",
        "        elif file_path.lower().endswith('.pdf'):\n",
        "            text = extract_text(file_path)\n",
        "        else:\n",
        "            raise ValueError(\"Format de fichier non supporté\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur d'extraction: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def clean_data(text):\n",
        "    \"\"\"Phase 2: Nettoyage et normalisation des données\"\"\"\n",
        "    # Suppression des caractères spéciaux\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?À-ÿ-]', ' ', text)\n",
        "    # Normalisation des espaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Conversion en minuscules\n",
        "    return text.lower().strip()\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"Phase 3: Reconnaissance d'entités nommées\"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"PERSON\": [],\n",
        "        \"ORG\": [],\n",
        "        \"DATE\": [],\n",
        "        \"LOC\": [],\n",
        "        \"DIPLOME\": [],\n",
        "        \"COMPETENCE\": []\n",
        "    }\n",
        "\n",
        "    # Entités standard\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entities and ent.text not in entities[ent.label_]:\n",
        "            entities[ent.label_].append(ent.text)\n",
        "\n",
        "    # Règles personnalisées pour diplômes et compétences\n",
        "    diplome_patterns = [\n",
        "        [{\"LOWER\": {\"IN\": [\"bac\", \"bts\", \"dut\", \"licence\", \"master\", \"doctorat\"]}}],\n",
        "        [{\"LOWER\": \"bachelor\"}, {\"LOWER\": \"of\"}, {\"LOWER\": \"science\"}],\n",
        "        [{\"LOWER\": \"ingénieur\"}]\n",
        "    ]\n",
        "\n",
        "    matcher = Matcher(nlp.vocab)\n",
        "    matcher.add(\"DIPLOME\", diplome_patterns)\n",
        "\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        if span.text not in entities[\"DIPLOME\"]:\n",
        "            entities[\"DIPLOME\"].append(span.text)\n",
        "\n",
        "    return entities\n",
        "\n",
        "def extract_relations(entities, text):\n",
        "    \"\"\"Phase 4: Extraction des relations entre entités\"\"\"\n",
        "    relations = []\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Exemple: Relation entre compétence et expérience\n",
        "    for sentence in doc.sents:\n",
        "        if \"expérience\" in sentence.text:\n",
        "            for competence in entities[\"COMPETENCE\"]:\n",
        "                if competence in sentence.text:\n",
        "                    relations.append({\n",
        "                        \"relation\": \"a_experience_avec\",\n",
        "                        \"entite\": competence,\n",
        "                        \"contexte\": sentence.text.strip()\n",
        "                    })\n",
        "\n",
        "    return relations\n",
        "\n",
        "def build_model(entities, relations):\n",
        "    \"\"\"Phase 5: Construction du modèle de données\"\"\"\n",
        "    model = {\n",
        "        \"candidat\": {\n",
        "            \"nom\": entities[\"PERSON\"][0] if entities[\"PERSON\"] else \"Inconnu\",\n",
        "            \"contact\": \"\",\n",
        "            \"education\": entities[\"DIPLOME\"],\n",
        "            \"competences\": entities[\"COMPETENCE\"],\n",
        "            \"experiences\": []\n",
        "        },\n",
        "        \"relations\": relations\n",
        "    }\n",
        "\n",
        "    # Extraction des expériences professionnelles\n",
        "    for org in entities[\"ORG\"]:\n",
        "        model[\"candidat\"][\"experiences\"].append({\n",
        "            \"organisation\": org,\n",
        "            \"poste\": \"À déterminer\",\n",
        "            \"duree\": \"\"\n",
        "        })\n",
        "\n",
        "    return model\n",
        "\n",
        "def display_results(model):\n",
        "    \"\"\"Phase 6: Présentation des résultats\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ANALYSE COMPLÈTE DU CV\".center(50))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"\\nCandidat: {model['candidat']['nom']}\")\n",
        "    print(\"\\nFormation:\")\n",
        "    for i, diplome in enumerate(model['candidat']['education'], 1):\n",
        "        print(f\"  {i}. {diplome}\")\n",
        "\n",
        "    print(\"\\nCompétences techniques:\")\n",
        "    for i, competence in enumerate(model['candidat']['competences'], 1):\n",
        "        print(f\"  {i}. {competence}\")\n",
        "\n",
        "    print(\"\\nExpériences professionnelles:\")\n",
        "    for i, exp in enumerate(model['candidat']['experiences'], 1):\n",
        "        print(f\"  {i}. {exp['organisation']} - {exp['poste']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Relations identifiées:\".center(50))\n",
        "    for rel in model['relations']:\n",
        "        print(f\"- {rel['entite']} ({rel['relation']})\")\n",
        "\n",
        "    # Export JSON\n",
        "    with open('cv_analysis.json', 'w') as f:\n",
        "        json.dump(model, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "z7_vlAY0OrpV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline complet\n",
        "def analyze_cv(file_path):\n",
        "    print(f\"Début de l'analyse du fichier: {file_path}\")\n",
        "\n",
        "    # Phase 1: Extraction\n",
        "    raw_text = extract_data(file_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    # Phase 2: Nettoyage\n",
        "    cleaned_text = clean_data(raw_text)\n",
        "\n",
        "    # Phase 3: Entités\n",
        "    entities = extract_entities(cleaned_text)\n",
        "\n",
        "    # Phase 4: Relations\n",
        "    relations = extract_relations(entities, cleaned_text)\n",
        "\n",
        "    # Phase 5: Modélisation\n",
        "    model = build_model(entities, relations)\n",
        "\n",
        "    # Phase 6: Présentation\n",
        "    return display_results(model)\n",
        "\n",
        "# Exemple d'utilisation\n",
        "if __name__ == \"__main__\":\n",
        "    cv_path = \"chemin/vers/votre/cv.pdf\"  # ou .jpg/.png\n",
        "    result = analyze_cv(cv_path)\n",
        "\n",
        "    if result:\n",
        "        print(\"\\n>>> Analyse terminée avec succès!\")\n",
        "        print(f\">>> Données exportées dans 'cv_analysis.json'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnsRTciLRfsD",
        "outputId": "0d13e18f-c740-459c-b0d0-1f96cb1fd0d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Début de l'analyse du fichier: chemin/vers/votre/cv.pdf\n",
            "Erreur d'extraction: [Errno 2] No such file or directory: 'chemin/vers/votre/cv.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aacdc62"
      },
      "source": [
        "# Task\n",
        "Extend the existing CV analysis system with a web front-end (Streamlit), a database for storing results, and additional ML models for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c0aa35"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries for Streamlit, database interaction, and additional ML models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9fac2b4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `sqlite3` is a built-in Python library and does not need to be installed via pip. The other packages, `streamlit` and `scikit-learn`, should still be installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d376e3e8",
        "outputId": "d368b4c5-c668-4727-c908-ac0229b1f67c"
      },
      "source": [
        "!pip install streamlit scikit-learn"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Using cached streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.46.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f0e0c95"
      },
      "source": [
        "## Integrate streamlit\n",
        "\n",
        "### Subtask:\n",
        "Create a basic Streamlit application to upload files and display initial results from the existing analysis functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3b11d4b"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the basic Streamlit application structure with file upload and initial result display by incorporating the existing analysis functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ad459ac",
        "outputId": "c8a132f5-6f55-41ad-af9a-0ccfd6ceae69"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "\n",
        "# Re-import necessary functions from the previous cells\n",
        "# Ensure the previous cells defining these functions (extract_data, clean_data,\n",
        "# extract_entities, extract_relations, build_model, display_results, analyze_cv)\n",
        "# have been executed.\n",
        "\n",
        "# Configuration de l'OCR (Tesseract) - needs to be set if not already\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>' # Replace with your Tesseract path\n",
        "\n",
        "st.title(\"Analyse de CV\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choisissez un fichier PDF ou image (PNG, JPG, JPEG)\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # To use the existing analyze_cv function, we need a file path.\n",
        "    # Streamlit's file_uploader provides an in-memory file object.\n",
        "    # We can save it to a temporary file to get a path.\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    st.info(f\"Fichier téléchargé: {uploaded_file.name}\")\n",
        "\n",
        "    # Call the existing analysis pipeline\n",
        "    st.text(\"Analyse en cours...\")\n",
        "    analysis_result = analyze_cv(tmp_file_path)\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.unlink(tmp_file_path)\n",
        "\n",
        "    if analysis_result:\n",
        "        st.success(\"Analyse terminée avec succès!\")\n",
        "\n",
        "        st.subheader(\"Résultats de l'analyse\")\n",
        "\n",
        "        st.markdown(\"#### Informations Candidat\")\n",
        "        st.write(f\"**Nom:** {analysis_result['candidat']['nom']}\")\n",
        "        # Add other basic info like contact if extracted later\n",
        "\n",
        "        st.markdown(\"#### Formation\")\n",
        "        if analysis_result['candidat']['education']:\n",
        "            for i, diplome in enumerate(analysis_result['candidat']['education'], 1):\n",
        "                st.write(f\"- {diplome}\")\n",
        "        else:\n",
        "            st.write(\"Aucune information de formation trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Compétences\")\n",
        "        if analysis_result['candidat']['competences']:\n",
        "            for i, competence in enumerate(analysis_result['candidat']['competences'], 1):\n",
        "                 st.write(f\"- {competence}\")\n",
        "        else:\n",
        "            st.write(\"Aucune compétence trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Expériences Professionnelles\")\n",
        "        if analysis_result['candidat']['experiences']:\n",
        "            for i, exp in enumerate(analysis_result['candidat']['experiences'], 1):\n",
        "                st.write(f\"- **Organisation:** {exp['organisation']}\")\n",
        "                # Add poste, duree if extracted later\n",
        "        else:\n",
        "            st.write(\"Aucune expérience professionnelle trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Relations identifiées\")\n",
        "        if analysis_result['relations']:\n",
        "            for rel in analysis_result['relations']:\n",
        "                 st.write(f\"- {rel['entite']} ({rel['relation']})\")\n",
        "        else:\n",
        "            st.write(\"Aucune relation identifiée.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        st.error(\"Échec de l'analyse du CV. Veuillez vérifier le fichier.\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-15 23:01:18.124 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.197 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-07-15 23:01:18.198 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:01:18.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5dd1f5"
      },
      "source": [
        "## Database setup\n",
        "\n",
        "### Subtask:\n",
        "Choose and set up a database (e.g., SQLite for simplicity, or PostgreSQL/MySQL for scalability). Define the schema for storing CV analysis results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "966f6139"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the database schema and write the Python code to connect to an SQLite database and create the necessary tables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d289702",
        "outputId": "105e6143-974e-4b25-fb7c-364c939ff6b6"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "def create_database_and_tables():\n",
        "    \"\"\"Connects to SQLite DB and creates tables if they don't exist.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create tables\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS candidates (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                name TEXT NOT NULL,\n",
        "                contact TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS education (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                degree TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS skills (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                skill TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS experiences (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                organization TEXT NOT NULL,\n",
        "                position TEXT,\n",
        "                duration TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS relations (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                relation_type TEXT,\n",
        "                entity TEXT,\n",
        "                context TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        print(f\"Database '{DATABASE_NAME}' and tables created successfully or already exist.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Execute the function to create the database and tables\n",
        "create_database_and_tables()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 'cv_analysis.db' and tables created successfully or already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb2588bd"
      },
      "source": [
        "## Integrate database\n",
        "\n",
        "### Subtask:\n",
        "Modify the Python code to store the analysis results (extracted entities, relations, etc.) into the database after processing a CV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a432862"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the sqlite3 library and define the `save_analysis_to_db` function to store the analysis results in the database. This function will handle connecting to the database, inserting data into the respective tables, and handling potential errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d546963"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "def save_analysis_to_db(model):\n",
        "    \"\"\"Saves the CV analysis results into the SQLite database.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Insert candidate information\n",
        "        candidate_name = model['candidat']['nom']\n",
        "        candidate_contact = model['candidat']['contact'] # Assuming contact will be added later\n",
        "        cursor.execute(\"INSERT INTO candidates (name, contact) VALUES (?, ?)\", (candidate_name, candidate_contact))\n",
        "        candidate_id = cursor.lastrowid\n",
        "\n",
        "        # Insert education\n",
        "        for degree in model['candidat']['education']:\n",
        "            cursor.execute(\"INSERT INTO education (candidate_id, degree) VALUES (?, ?)\", (candidate_id, degree))\n",
        "\n",
        "        # Insert skills\n",
        "        for skill in model['candidat']['competences']:\n",
        "            cursor.execute(\"INSERT INTO skills (candidate_id, skill) VALUES (?, ?)\", (candidate_id, skill))\n",
        "\n",
        "        # Insert experiences\n",
        "        for exp in model['candidat']['experiences']:\n",
        "            organization = exp.get('organisation', 'N/A')\n",
        "            position = exp.get('poste', 'À déterminer')\n",
        "            duration = exp.get('duree', '')\n",
        "            cursor.execute(\"INSERT INTO experiences (candidate_id, organization, position, duration) VALUES (?, ?, ?, ?)\",\n",
        "                           (candidate_id, organization, position, duration))\n",
        "\n",
        "        # Insert relations\n",
        "        for rel in model['relations']:\n",
        "            relation_type = rel.get('relation', 'N/A')\n",
        "            entity = rel.get('entite', 'N/A')\n",
        "            context = rel.get('contexte', '')\n",
        "            cursor.execute(\"INSERT INTO relations (candidate_id, relation_type, entity, context) VALUES (?, ?, ?, ?)\",\n",
        "                           (candidate_id, relation_type, entity, context))\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"Analysis results saved to database.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error while saving analysis: {e}\")\n",
        "        if conn:\n",
        "            conn.rollback() # Rollback changes if an error occurs\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3355b4a5"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `analyze_cv` function to call `save_analysis_to_db` after successfully building the model and before displaying the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5242a74",
        "outputId": "b8c97d66-591b-4590-970a-0edc3956e982"
      },
      "source": [
        "# Pipeline complet\n",
        "def analyze_cv(file_path):\n",
        "    print(f\"Début de l'analyse du fichier: {file_path}\")\n",
        "\n",
        "    # Phase 1: Extraction\n",
        "    raw_text = extract_data(file_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    # Phase 2: Nettoyage\n",
        "    cleaned_text = clean_data(raw_text)\n",
        "\n",
        "    # Phase 3: Entités\n",
        "    entities = extract_entities(cleaned_text)\n",
        "\n",
        "    # Phase 4: Relations\n",
        "    relations = extract_relations(entities, cleaned_text)\n",
        "\n",
        "    # Phase 5: Modélisation\n",
        "    model = build_model(entities, relations)\n",
        "\n",
        "    # Phase 6: Save to database\n",
        "    save_analysis_to_db(model)\n",
        "\n",
        "    # Phase 7: Présentation (still call the original display function)\n",
        "    return display_results(model)\n",
        "\n",
        "# Example d'utilisation (Keep for testing, but the Streamlit app will use this function)\n",
        "if __name__ == \"__main__\":\n",
        "    cv_path = \"chemin/vers/votre/cv.pdf\"  # ou .jpg/.png\n",
        "    result = analyze_cv(cv_path)\n",
        "\n",
        "    if result:\n",
        "        print(\"\\n>>> Analyse terminée avec succès!\")\n",
        "        print(f\">>> Données exportées dans 'cv_analysis.json' et sauvegardées dans '{DATABASE_NAME}'\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Début de l'analyse du fichier: chemin/vers/votre/cv.pdf\n",
            "Erreur d'extraction: [Errno 2] No such file or directory: 'chemin/vers/votre/cv.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17eb23de"
      },
      "source": [
        "## Implement database interaction in streamlit\n",
        "\n",
        "### Subtask:\n",
        "Add functionality to the Streamlit app to retrieve and display analysis results from the database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ed07f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for the Streamlit app and database interaction, and define the function to retrieve analysis results from the database.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "867a4ce9",
        "outputId": "d7877708-f27f-478a-9c93-1add5cd5d7d5"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import sqlite3\n",
        "\n",
        "# Assuming DATABASE_NAME is defined in a previous cell\n",
        "# DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "# Ensure the previous cells defining the analysis functions (extract_data, clean_data,\n",
        "# extract_entities, extract_relations, build_model, display_results, analyze_cv,\n",
        "# save_analysis_to_db, create_database_and_tables) have been executed.\n",
        "\n",
        "# Configuration de l'OCR (Tesseract) - needs to be set if not already\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>' # Replace with your Tesseract path\n",
        "\n",
        "def get_analysis_from_db(candidate_name):\n",
        "    \"\"\"Queries the database for analysis results based on the candidate's name.\"\"\"\n",
        "    conn = None\n",
        "    analysis_data = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Get candidate ID\n",
        "        cursor.execute(\"SELECT id, name, contact FROM candidates WHERE name = ?\", (candidate_name,))\n",
        "        candidate_info = cursor.fetchone()\n",
        "\n",
        "        if candidate_info:\n",
        "            candidate_id, name, contact = candidate_info\n",
        "\n",
        "            analysis_data = {\n",
        "                \"candidat\": {\n",
        "                    \"nom\": name,\n",
        "                    \"contact\": contact,\n",
        "                    \"education\": [],\n",
        "                    \"competences\": [],\n",
        "                    \"experiences\": []\n",
        "                },\n",
        "                \"relations\": []\n",
        "            }\n",
        "\n",
        "            # Get education\n",
        "            cursor.execute(\"SELECT degree FROM education WHERE candidate_id = ?\", (candidate_id,))\n",
        "            education_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"education\"] = [row[0] for row in education_results]\n",
        "\n",
        "            # Get skills\n",
        "            cursor.execute(\"SELECT skill FROM skills WHERE candidate_id = ?\", (candidate_id,))\n",
        "            skills_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"competences\"] = [row[0] for row in skills_results]\n",
        "\n",
        "            # Get experiences\n",
        "            cursor.execute(\"SELECT organization, position, duration FROM experiences WHERE candidate_id = ?\", (candidate_id,))\n",
        "            experiences_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"experiences\"] = [{\"organisation\": row[0], \"poste\": row[1], \"duree\": row[2]} for row in experiences_results]\n",
        "\n",
        "            # Get relations\n",
        "            cursor.execute(\"SELECT relation_type, entity, context FROM relations WHERE candidate_id = ?\", (candidate_id,))\n",
        "            relations_results = cursor.fetchall()\n",
        "            analysis_data[\"relations\"] = [{\"relation\": row[0], \"entite\": row[1], \"contexte\": row[2]} for row in relations_results]\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        st.error(f\"Database error while retrieving analysis: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "    return analysis_data\n",
        "\n",
        "# Create tables if they don't exist when the app starts\n",
        "create_database_and_tables()\n",
        "\n",
        "st.title(\"Analyse de CV\")\n",
        "\n",
        "# Section for uploading a new CV\n",
        "st.header(\"Analyser un nouveau CV\")\n",
        "uploaded_file = st.file_uploader(\"Choisissez un fichier PDF ou image (PNG, JPG, JPEG)\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    st.info(f\"Fichier téléchargé: {uploaded_file.name}\")\n",
        "\n",
        "    st.text(\"Analyse en cours...\")\n",
        "    # analyze_cv now saves to DB\n",
        "    analysis_result = analyze_cv(tmp_file_path)\n",
        "\n",
        "    # Retrieve data from the database after saving\n",
        "    if analysis_result and 'candidat' in analysis_result and 'nom' in analysis_result['candidat']:\n",
        "        candidate_name = analysis_result['candidat']['nom']\n",
        "        db_analysis_data = get_analysis_from_db(candidate_name)\n",
        "        if db_analysis_data:\n",
        "            st.success(f\"Analyse terminée et sauvegardée pour {candidate_name}!\")\n",
        "            # Display data retrieved from the database\n",
        "            st.subheader(f\"Résultats de l'analyse pour {candidate_name}\")\n",
        "            display_results(db_analysis_data) # Reuse the display function, passing data from DB\n",
        "        else:\n",
        "            st.warning(f\"Analyse terminée, mais les données pour {candidate_name} n'ont pas pu être récupérées de la base de données.\")\n",
        "\n",
        "    else:\n",
        "        st.error(\"Échec de l'analyse du CV. Veuillez vérifier le fichier.\")\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.unlink(tmp_file_path)\n",
        "\n",
        "\n",
        "# Section for viewing previously analyzed CVs\n",
        "st.header(\"Voir les analyses précédentes\")\n",
        "\n",
        "# Get list of candidates from the database\n",
        "conn = None\n",
        "candidate_names = []\n",
        "try:\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM candidates ORDER BY name\")\n",
        "    candidate_names = [row[0] for row in cursor.fetchall()]\n",
        "except sqlite3.Error as e:\n",
        "    st.error(f\"Database error while fetching candidate list: {e}\")\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n",
        "\n",
        "if candidate_names:\n",
        "    selected_candidate = st.selectbox(\"Sélectionnez un candidat:\", [\"-- Sélectionner --\"] + candidate_names)\n",
        "\n",
        "    if selected_candidate != \"-- Sélectionner --\":\n",
        "        st.text(f\"Récupération de l'analyse pour {selected_candidate}...\")\n",
        "        previous_analysis_data = get_analysis_from_db(selected_candidate)\n",
        "\n",
        "        if previous_analysis_data:\n",
        "            st.subheader(f\"Résultats de l'analyse pour {selected_candidate}\")\n",
        "            display_results(previous_analysis_data) # Reuse the display function\n",
        "        else:\n",
        "            st.warning(f\"Aucune analyse trouvée pour {selected_candidate}.\")\n",
        "else:\n",
        "    st.info(\"Aucun CV analysé n'a été trouvé dans la base de données.\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-15 23:03:53.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.896 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.897 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.901 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.904 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.904 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.905 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.907 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.908 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:03:53.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 'cv_analysis.db' and tables created successfully or already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b06b91e"
      },
      "source": [
        "## Research and select additional ml models\n",
        "\n",
        "### Subtask:\n",
        "Identify suitable ML models for CV classification (e.g., classifying candidates by role, experience level, etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e058fdb"
      },
      "source": [
        "**Reasoning**:\n",
        "Research and identify suitable ML models for CV classification based on common approaches for text classification, considering their applicability to CV data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c29a67d",
        "outputId": "8d4ee011-780f-4d23-9d11-c9a57ccf27ca"
      },
      "source": [
        "# 1. Research common machine learning approaches for text classification,\n",
        "#    specifically applied to CVs or resumes.\n",
        "\n",
        "# Common approaches for text classification:\n",
        "# - Traditional ML: Naive Bayes, SVM, Logistic Regression, Decision Trees, Random Forests\n",
        "# - Feature Extraction for Traditional ML: Bag-of-Words (BoW), TF-IDF, Word Embeddings (used with pooling)\n",
        "# - Deep Learning: Recurrent Neural Networks (RNNs like LSTMs, GRUs), Convolutional Neural Networks (CNNs),\n",
        "#                  Transformer-based models (BERT, RoBERTa, etc.)\n",
        "# - Feature Extraction for Deep Learning: Word Embeddings (Word2Vec, GloVe, FastText), Contextual Embeddings (BERT, etc.)\n",
        "\n",
        "# Applications to CVs:\n",
        "# - Classifying job roles (e.g., Software Engineer, Data Scientist, Marketing Manager)\n",
        "# - Classifying experience levels (e.g., Junior, Mid-level, Senior)\n",
        "# - Classifying industry\n",
        "# - Classifying required skills/qualifications\n",
        "\n",
        "# 2. Identify at least two different types of models that could be suitable for classifying CVs.\n",
        "\n",
        "# Model Type 1: Traditional ML (e.g., SVM with TF-IDF features)\n",
        "# Reasoning:\n",
        "# - Strengths: Relatively simple to understand and implement, often perform well on text classification tasks with appropriate feature engineering (like TF-IDF), computationally less expensive than deep learning models for training.\n",
        "# - Applicability to CVs: Can effectively capture the importance of specific terms (skills, job titles, degree names) using TF-IDF, which are highly relevant for CV classification. SVMs are known for their effectiveness in high-dimensional spaces like text data.\n",
        "\n",
        "# Model Type 2: Deep Learning (e.g., Transformer-based model like BERT)\n",
        "# Reasoning:\n",
        "# - Strengths: Capable of capturing complex patterns, context, and semantic relationships in text data, which is crucial for understanding the nuances in a CV. Pre-trained transformer models (like BERT) can leverage vast amounts of external text data, requiring less domain-specific data for fine-tuning.\n",
        "# - Applicability to CVs: Can understand the context in which skills, experiences, and education are presented, potentially leading to more accurate classification than models relying solely on word frequency (TF-IDF). Can handle variations in phrasing and structure.\n",
        "\n",
        "# 3. Briefly explain the reasoning behind selecting these models.\n",
        "# (Explanation is included in the reasoning section above for each model type)\n",
        "\n",
        "# 4. Do not implement or train the models at this stage, only research and select.\n",
        "# (This step is adhered to)\n",
        "\n",
        "print(\"Selected ML Models for CV Classification:\")\n",
        "print(\"- Traditional ML: Support Vector Machine (SVM) with TF-IDF features\")\n",
        "print(\"  Reasoning: Good performance on text classification, captures term importance.\")\n",
        "print(\"- Deep Learning: Transformer-based model (e.g., BERT)\")\n",
        "print(\"  Reasoning: Captures context and semantic relationships, leverages pre-training on large text corpora.\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected ML Models for CV Classification:\n",
            "- Traditional ML: Support Vector Machine (SVM) with TF-IDF features\n",
            "  Reasoning: Good performance on text classification, captures term importance.\n",
            "- Deep Learning: Transformer-based model (e.g., BERT)\n",
            "  Reasoning: Captures context and semantic relationships, leverages pre-training on large text corpora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe462118"
      },
      "source": [
        "## Train and evaluate ml models\n",
        "\n",
        "### Subtask:\n",
        "Prepare a dataset, train the selected ML models, and evaluate their performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "442193c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires preparing a dataset, training an SVM model with TF-IDF features, and evaluating its performance. This involves loading/creating data, preprocessing text, vectorizing using TF-IDF, splitting data, training SVM, and evaluating. I will combine these steps into a single code block. Since a dataset is not provided, I will create a synthetic one for demonstration purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf4f51d",
        "outputId": "820f36db-f526-41bf-f03f-515e9d399cad"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load or create a dataset (creating a synthetic one)\n",
        "data = {\n",
        "    'cv_text': [\n",
        "        \"Experienced software engineer with skills in Python, Java, and SQL. Worked at Google for 5 years. Master's degree in Computer Science.\",\n",
        "        \"Marketing manager with expertise in digital marketing, social media, and content creation. 3 years experience at Facebook. Bachelor's degree in Marketing.\",\n",
        "        \"Data scientist with strong background in machine learning, statistics, and R. PhD in Statistics from Stanford.\",\n",
        "        \"Junior software developer with knowledge of C++ and algorithms. Internship at Microsoft. Pursuing Bachelor's degree.\",\n",
        "        \"Senior marketing specialist with focus on SEO and analytics. 10 years experience in the industry. MBA.\",\n",
        "        \"Recent graduate with a Bachelor's in Computer Science. Skills: Python, HTML, CSS. Looking for entry-level software engineering roles.\",\n",
        "        \"Experienced project manager with PMP certification. Managed large teams at Amazon. Background in engineering.\",\n",
        "        \"Graphic designer with proficiency in Adobe Creative Suite. Portfolio available. Bachelor's degree in Fine Arts.\",\n",
        "        \"Business analyst with experience in data analysis and strategic planning. Worked in consulting for 4 years. Master's in Business Administration.\",\n",
        "        \"Entry-level data analyst with skills in Excel and SQL. Internship experience. Bachelor's degree in Economics.\"\n",
        "    ],\n",
        "    'job_role': [\n",
        "        'Software Engineer', 'Marketing', 'Data Scientist', 'Software Engineer', 'Marketing',\n",
        "        'Software Engineer', 'Project Manager', 'Graphic Designer', 'Business Analyst', 'Data Analyst'\n",
        "    ],\n",
        "    'experience_level': [\n",
        "        'Senior', 'Mid-level', 'Senior', 'Junior', 'Senior',\n",
        "        'Junior', 'Senior', 'Entry-level', 'Mid-level', 'Entry-level'\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# For this subtask, we will classify by 'job_role'\n",
        "X = df['cv_text']\n",
        "y = df['job_role']\n",
        "\n",
        "# 2. Preprocess text data (tokenization, lowercase, remove stop words)\n",
        "# TF-IDF Vectorizer handles tokenization, lowercase, and can remove stop words\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "\n",
        "# 3. Convert preprocessed text data into numerical features using TF-IDF vectorization.\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Split the dataset into training and testing sets.\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 5. Initialize and train the SVM model on the training data using the TF-IDF features.\n",
        "svm_model = SVC(kernel='linear') # Using linear kernel for simplicity and often good performance on text\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 6. Evaluate the performance of the trained SVM model on the testing data.\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"SVM Model Performance (TF-IDF Features):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Note: The dataset is very small and synthetic, so performance metrics will not be representative of real-world scenarios.\n",
        "# This serves as a demonstration of the steps.\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model Performance (TF-IDF Features):\n",
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            " Business Analyst       0.00      0.00      0.00         1\n",
            "        Marketing       0.00      0.00      0.00         1\n",
            "Software Engineer       0.33      1.00      0.50         1\n",
            "\n",
            "         accuracy                           0.33         3\n",
            "        macro avg       0.11      0.33      0.17         3\n",
            "     weighted avg       0.11      0.33      0.17         3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9800067"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step completed the SVM model training and evaluation. The next steps in the subtask involve working with a Deep Learning model (BERT). This requires installing necessary libraries, preprocessing data using a BERT tokenizer, preparing the data for BERT, splitting the data, loading a pre-trained BERT model, fine-tuning it, and evaluating its performance. I will combine the installation and the BERT tokenization and data preparation steps into this code block. Since we need to fine-tune BERT, we'll use the `transformers` library from Hugging Face and `torch` or `tensorflow` for the deep learning framework. I will use `torch`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0zwgn682VWdi",
        "outputId": "e6703c6e-25c1-49f1-f73d-c751b8f09644"
      },
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.functional import cross_entropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd # Re-import just in case, though likely available\n",
        "\n",
        "# Assuming df, X, y are already defined from the previous step\n",
        "\n",
        "# 7. Preprocess the text data by tokenizing it using a BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Map labels to integers\n",
        "label_map = {label: i for i, label in enumerate(y.unique())}\n",
        "y_encoded = y.map(label_map)\n",
        "\n",
        "# 8. Prepare the data in a format suitable for training a BERT model\n",
        "class CVDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length', # Pad to max_len\n",
        "            truncation=True, # Truncate if longer than max_len\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt', # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define max sequence length (adjust based on typical CV length)\n",
        "MAX_LEN = 256 # Example length\n",
        "\n",
        "# 9. Split the dataset into training and testing sets.\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded) # Stratify to maintain class distribution\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CVDataset(\n",
        "    texts=X_train.tolist(),\n",
        "    labels=y_train_encoded.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "\n",
        "test_dataset = CVDataset(\n",
        "    texts=X_test.tolist(),\n",
        "    labels=y_test_encoded.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "print(f\"Number of unique labels: {len(label_map)}\")\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.8.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.7.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.9)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-3697680080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install accelerate -U'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "370c23f6"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries for Streamlit, database interaction, and additional ML models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08e10e01",
        "outputId": "5bd7bc0d-eb69-4ca3-c53d-185659b9d1fe"
      },
      "source": [
        "# Install Streamlit and scikit-learn\n",
        "!pip install streamlit scikit-learn\n",
        "# sqlite3 is a built-in Python library, so no installation is needed for it."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fde58698"
      },
      "source": [
        "## Integrate streamlit\n",
        "\n",
        "### Subtask:\n",
        "Create a basic Streamlit application to upload files and display initial results from the existing analysis functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59541d4c"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the basic Streamlit application structure with file upload and initial result display by incorporating the existing analysis functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e63bf8a",
        "outputId": "3fd250f8-7bbf-4eb1-cd5e-e445b3a0a333"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "\n",
        "# Re-import necessary functions from the previous cells\n",
        "# Ensure the previous cells defining these functions (extract_data, clean_data,\n",
        "# extract_entities, extract_relations, build_model, display_results, analyze_cv)\n",
        "# have been executed.\n",
        "\n",
        "# Configuration de l'OCR (Tesseract) - needs to be set if not already\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>' # Replace with your Tesseract path\n",
        "\n",
        "st.title(\"Analyse de CV\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choisissez un fichier PDF ou image (PNG, JPG, JPEG)\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # To use the existing analyze_cv function, we need a file path.\n",
        "    # Streamlit's file_uploader provides an in-memory file object.\n",
        "    # We can save it to a temporary file to get a path.\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    st.info(f\"Fichier téléchargé: {uploaded_file.name}\")\n",
        "\n",
        "    # Call the existing analysis pipeline\n",
        "    st.text(\"Analyse en cours...\")\n",
        "    analysis_result = analyze_cv(tmp_file_path)\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.unlink(tmp_file_path)\n",
        "\n",
        "    if analysis_result:\n",
        "        st.success(\"Analyse terminée avec succès!\")\n",
        "\n",
        "        st.subheader(\"Résultats de l'analyse\")\n",
        "\n",
        "        st.markdown(\"#### Informations Candidat\")\n",
        "        st.write(f\"**Nom:** {analysis_result['candidat']['nom']}\")\n",
        "        # Add other basic info like contact if extracted later\n",
        "\n",
        "        st.markdown(\"#### Formation\")\n",
        "        if analysis_result['candidat']['education']:\n",
        "            for i, diplome in enumerate(analysis_result['candidat']['education'], 1):\n",
        "                st.write(f\"- {diplome}\")\n",
        "        else:\n",
        "            st.write(\"Aucune information de formation trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Compétences\")\n",
        "        if analysis_result['candidat']['competences']:\n",
        "            for i, competence in enumerate(analysis_result['candidat']['competences'], 1):\n",
        "                 st.write(f\"- {competence}\")\n",
        "        else:\n",
        "            st.write(\"Aucune compétence trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Expériences Professionnelles\")\n",
        "        if analysis_result['candidat']['experiences']:\n",
        "            for i, exp in enumerate(analysis_result['candidat']['experiences'], 1):\n",
        "                st.write(f\"- **Organisation:** {exp['organisation']}\")\n",
        "                # Add poste, duree if extracted later\n",
        "        else:\n",
        "            st.write(\"Aucune expérience professionnelle trouvée.\")\n",
        "\n",
        "        st.markdown(\"#### Relations identifiées\")\n",
        "        if analysis_result['relations']:\n",
        "            for rel in analysis_result['relations']:\n",
        "                 st.write(f\"- {rel['entite']} ({rel['relation']})\")\n",
        "        else:\n",
        "            st.write(\"Aucune relation identifiée.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        st.error(\"Échec de l'analyse du CV. Veuillez vérifier le fichier.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-15 23:11:25.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.695 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:11:25.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2923886"
      },
      "source": [
        "## Database setup\n",
        "\n",
        "### Subtask:\n",
        "Choose and set up a database (e.g., SQLite for simplicity, or PostgreSQL/MySQL for scalability). Define the schema for storing CV analysis results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4295d358"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the database schema and write the Python code to connect to an SQLite database and create the necessary tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fc5750f",
        "outputId": "c4c13fc5-b6e3-44bd-e131-c7eb3a198df2"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "def create_database_and_tables():\n",
        "    \"\"\"Connects to SQLite DB and creates tables if they don't exist.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create tables\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS candidates (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                name TEXT NOT NULL,\n",
        "                contact TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS education (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                degree TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS skills (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                skill TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS experiences (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                organization TEXT NOT NULL,\n",
        "                position TEXT,\n",
        "                duration TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS relations (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                relation_type TEXT,\n",
        "                entity TEXT,\n",
        "                context TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        print(f\"Database '{DATABASE_NAME}' and tables created successfully or already exist.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Execute the function to create the database and tables\n",
        "create_database_and_tables()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 'cv_analysis.db' and tables created successfully or already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbd3774"
      },
      "source": [
        "## Integrate database\n",
        "\n",
        "### Subtask:\n",
        "Modify the Python code to store the analysis results (extracted entities, relations, etc.) into the database after processing a CV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b424828"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the sqlite3 library and define the `save_analysis_to_db` function to store the analysis results in the database. This function will handle connecting to the database, inserting data into the respective tables, and handling potential errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09b0ecf4"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "def save_analysis_to_db(model):\n",
        "    \"\"\"Saves the CV analysis results into the SQLite database.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Insert candidate information\n",
        "        candidate_name = model['candidat']['nom']\n",
        "        candidate_contact = model['candidat']['contact'] # Assuming contact will be added later\n",
        "        cursor.execute(\"INSERT INTO candidates (name, contact) VALUES (?, ?)\", (candidate_name, candidate_contact))\n",
        "        candidate_id = cursor.lastrowid\n",
        "\n",
        "        # Insert education\n",
        "        for degree in model['candidat']['education']:\n",
        "            cursor.execute(\"INSERT INTO education (candidate_id, degree) VALUES (?, ?)\", (candidate_id, degree))\n",
        "\n",
        "        # Insert skills\n",
        "        for skill in model['candidat']['competences']:\n",
        "            cursor.execute(\"INSERT INTO skills (candidate_id, skill) VALUES (?, ?)\", (candidate_id, skill))\n",
        "\n",
        "        # Insert experiences\n",
        "        for exp in model['candidat']['experiences']:\n",
        "            organization = exp.get('organisation', 'N/A')\n",
        "            position = exp.get('poste', 'À déterminer')\n",
        "            duration = exp.get('duree', '')\n",
        "            cursor.execute(\"INSERT INTO experiences (candidate_id, organization, position, duration) VALUES (?, ?, ?, ?)\",\n",
        "                           (candidate_id, organization, position, duration))\n",
        "\n",
        "        # Insert relations\n",
        "        for rel in model['relations']:\n",
        "            relation_type = rel.get('relation', 'N/A')\n",
        "            entity = rel.get('entite', 'N/A')\n",
        "            context = rel.get('contexte', '')\n",
        "            cursor.execute(\"INSERT INTO relations (candidate_id, relation_type, entity, context) VALUES (?, ?, ?, ?)\",\n",
        "                           (candidate_id, relation_type, entity, context))\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"Analysis results saved to database.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error while saving analysis: {e}\")\n",
        "        if conn:\n",
        "            conn.rollback() # Rollback changes if an error occurs\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc69a761"
      },
      "source": [
        "## Implement database interaction in streamlit\n",
        "\n",
        "### Subtask:\n",
        "Add functionality to the Streamlit app to retrieve and display analysis results from the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9fda0b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for the Streamlit app and database interaction, and define the function to retrieve analysis results from the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c8571a5",
        "outputId": "6df6f2f3-e758-45a8-9875-c83b8549677e"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import sqlite3\n",
        "\n",
        "# Assuming DATABASE_NAME is defined in a previous cell\n",
        "DATABASE_NAME = 'cv_analysis.db' # Ensure DATABASE_NAME is defined\n",
        "\n",
        "# Ensure the previous cells defining the analysis functions (extract_data, clean_data,\n",
        "# extract_entities, extract_relations, build_model, display_results, analyze_cv,\n",
        "# save_analysis_to_db, create_database_and_tables, update_database_schema) have been executed.\n",
        "\n",
        "# Configuration de l'OCR (Tesseract) - needs to be set if not already\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>' # Replace with your Tesseract path\n",
        "\n",
        "\n",
        "def get_analysis_from_db(candidate_name):\n",
        "    \"\"\"Queries the database for analysis results based on the candidate's name.\"\"\"\n",
        "    conn = None\n",
        "    analysis_data = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Get candidate ID and classification results\n",
        "        cursor.execute(\"SELECT id, name, contact, job_role_svm, job_role_bert, experience_level_svm, experience_level_bert FROM candidates WHERE name = ?\", (candidate_name,))\n",
        "        candidate_info = cursor.fetchone()\n",
        "\n",
        "        if candidate_info:\n",
        "            candidate_id, name, contact, job_role_svm, job_role_bert, experience_level_svm, experience_level_bert = candidate_info\n",
        "\n",
        "            analysis_data = {\n",
        "                \"candidat\": {\n",
        "                    \"nom\": name,\n",
        "                    \"contact\": contact,\n",
        "                    \"education\": [],\n",
        "                    \"competences\": [],\n",
        "                    \"experiences\": [],\n",
        "                    \"job_role_svm\": job_role_svm, # Include classification results\n",
        "                    \"job_role_bert\": job_role_bert,\n",
        "                    \"experience_level_svm\": experience_level_svm,\n",
        "                    \"experience_level_bert\": experience_level_bert\n",
        "                },\n",
        "                \"relations\": []\n",
        "            }\n",
        "\n",
        "            # Get education\n",
        "            cursor.execute(\"SELECT degree FROM education WHERE candidate_id = ?\", (candidate_id,))\n",
        "            education_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"education\"] = [row[0] for row in education_results]\n",
        "\n",
        "            # Get skills\n",
        "            cursor.execute(\"SELECT skill FROM skills WHERE candidate_id = ?\", (candidate_id,))\n",
        "            skills_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"competences\"] = [row[0] for row in skills_results]\n",
        "\n",
        "            # Get experiences\n",
        "            cursor.execute(\"SELECT organization, position, duration FROM experiences WHERE candidate_id = ?\", (candidate_id,))\n",
        "            experiences_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"experiences\"] = [{\"organisation\": row[0], \"poste\": row[1], \"duree\": row[2]} for row in experiences_results]\n",
        "\n",
        "            # Get relations\n",
        "            cursor.execute(\"SELECT relation_type, entity, context FROM relations WHERE candidate_id = ?\", (candidate_id,))\n",
        "            relations_results = cursor.fetchall()\n",
        "            analysis_data[\"relations\"] = [{\"relation\": row[0], \"entite\": row[1], \"contexte\": row[2]} for row in relations_results]\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        st.error(f\"Database error while retrieving analysis: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "    return analysis_data\n",
        "\n",
        "# Create tables and update schema if they don't exist when the app starts\n",
        "create_database_and_tables()\n",
        "update_database_schema()\n",
        "\n",
        "\n",
        "st.title(\"Analyse de CV\")\n",
        "\n",
        "# Section for uploading a new CV\n",
        "st.header(\"Analyser un nouveau CV\")\n",
        "uploaded_file = st.file_uploader(\"Choisissez un fichier PDF ou image (PNG, JPG, JPEG)\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    st.info(f\"Fichier téléchargé: {uploaded_file.name}\")\n",
        "\n",
        "    st.text(\"Analyse en cours...\")\n",
        "    # analyze_cv now saves to DB and includes classification (assuming models are available)\n",
        "    # Note: The analyze_cv function now requires model objects.\n",
        "    # For this Streamlit app to work, you would need to load the trained models here\n",
        "    # or make them globally accessible. This is a simplification for demonstration.\n",
        "    # Assuming svm_model, tfidf_vectorizer, model (bert_model), tokenizer, label_map\n",
        "    # are available from previous cells' execution.\n",
        "    try:\n",
        "         # Pass the models and label_map to analyze_cv\n",
        "        analysis_result = analyze_cv(tmp_file_path, svm_model, tfidf_vectorizer, model, tokenizer, label_map)\n",
        "\n",
        "         # Retrieve data from the database after saving\n",
        "        if analysis_result and 'candidat' in analysis_result and 'nom' in analysis_result['candidat']:\n",
        "            candidate_name = analysis_result['candidat']['nom']\n",
        "            db_analysis_data = get_analysis_from_db(candidate_name)\n",
        "            if db_analysis_data:\n",
        "                st.success(f\"Analyse terminée et sauvegardée pour {candidate_name}!\")\n",
        "                # Display data retrieved from the database\n",
        "                st.subheader(f\"Résultats de l'analyse pour {candidate_name}\")\n",
        "                # Modify display_results to handle and show classification\n",
        "                # For now, we'll just display the classification results directly here\n",
        "                display_results(db_analysis_data) # Reuse the original display function\n",
        "\n",
        "                st.markdown(\"#### Classification Results\")\n",
        "                st.write(f\"**Job Role (SVM):** {db_analysis_data['candidat'].get('job_role_svm', 'N/A')}\")\n",
        "                st.write(f\"**Job Role (BERT):** {db_analysis_data['candidat'].get('job_role_bert', 'N/A')}\")\n",
        "                st.write(f\"**Experience Level (SVM):** {db_analysis_data['candidat'].get('experience_level_svm', 'N/A')}\")\n",
        "                st.write(f\"**Experience Level (BERT):** {db_analysis_data['candidat'].get('experience_level_bert', 'N/A')}\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                st.warning(f\"Analyse terminée, mais les données pour {candidate_name} n'ont pas pu être récupérées de la base de données.\")\n",
        "\n",
        "        else:\n",
        "            st.error(\"Échec de l'analyse du CV. Veuillez vérifier le file content or analysis logic.\")\n",
        "\n",
        "    except NameError as e:\n",
        "        st.error(f\"Error: Required models or variables are not defined. Please ensure all previous cells, including model training, have been executed. Details: {e}\")\n",
        "    except Exception as e:\n",
        "         st.error(f\"An unexpected error occurred during analysis: {e}\")\n",
        "\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.unlink(tmp_file_path)\n",
        "\n",
        "\n",
        "# Section for viewing previously analyzed CVs\n",
        "st.header(\"Voir les analyses précédentes\")\n",
        "\n",
        "# Get list of candidates from the database\n",
        "conn = None\n",
        "candidate_names = []\n",
        "try:\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM candidates ORDER BY name\")\n",
        "    candidate_names = [row[0] for row in cursor.fetchall()]\n",
        "except sqlite3.Error as e:\n",
        "    st.error(f\"Database error while fetching candidate list: {e}\")\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n",
        "\n",
        "if candidate_names:\n",
        "    selected_candidate = st.selectbox(\"Sélectionnez un candidat:\", [\"-- Sélectionner --\"] + candidate_names)\n",
        "\n",
        "    if selected_candidate != \"-- Sélectionner --\":\n",
        "        st.text(f\"Récupération de l'analyse pour {selected_candidate}...\")\n",
        "        previous_analysis_data = get_analysis_from_db(selected_candidate)\n",
        "\n",
        "        if previous_analysis_data:\n",
        "            st.subheader(f\"Résultats de l'analyse pour {selected_candidate}\")\n",
        "            display_results(previous_analysis_data) # Reuse the display function\n",
        "\n",
        "            st.markdown(\"#### Classification Results\")\n",
        "            st.write(f\"**Job Role (SVM):** {previous_analysis_data['candidat'].get('job_role_svm', 'N/A')}\")\n",
        "            st.write(f\"**Job Role (BERT):** {previous_analysis_data['candidat'].get('job_role_bert', 'N/A')}\")\n",
        "            st.write(f\"**Experience Level (SVM):** {previous_analysis_data['candidat'].get('experience_level_svm', 'N/A')}\")\n",
        "            st.write(f\"**Experience Level (BERT):** {previous_analysis_data['candidat'].get('experience_level_bert', 'N/A')}\")\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"Aucune analyse trouvée pour {selected_candidate}.\")\n",
        "else:\n",
        "    st.info(\"Aucun CV analysé n'a été trouvé dans la base de données.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-15 23:19:00.424 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.436 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.438 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.448 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.449 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.450 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.451 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.453 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.454 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.455 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-07-15 23:19:00.459 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 'cv_analysis.db' and tables created successfully or already exist.\n",
            "'job_role_svm' column already exists.\n",
            "'job_role_bert' column already exists.\n",
            "'experience_level_svm' column already exists.\n",
            "'experience_level_bert' column already exists.\n",
            "Database schema update process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88dfd134"
      },
      "source": [
        "## Research and select additional ml models\n",
        "\n",
        "### Subtask:\n",
        "Identify suitable ML models for CV classification (e.g., classifying candidates by role, experience level, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b30bfaf"
      },
      "source": [
        "**Reasoning**:\n",
        "Research and identify suitable ML models for CV classification based on common approaches for text classification, considering their applicability to CV data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b561e184",
        "outputId": "418e419d-a449-4897-89ea-1ec366227cbe"
      },
      "source": [
        "# 1. Research common machine learning approaches for text classification,\n",
        "#    specifically applied to CVs or resumes.\n",
        "\n",
        "# Common approaches for text classification:\n",
        "# - Traditional ML: Naive Bayes, SVM, Logistic Regression, Decision Trees, Random Forests\n",
        "# - Feature Extraction for Traditional ML: Bag-of-Words (BoW), TF-IDF, Word Embeddings (used with pooling)\n",
        "# - Deep Learning: Recurrent Neural Networks (RNNs like LSTMs, GRUs), Convolutional Neural Networks (CNNs),\n",
        "#                  Transformer-based models (BERT, RoBERTa, etc.)\n",
        "# - Feature Extraction for Deep Learning: Word Embeddings (Word2Vec, GloVe, FastText), Contextual Embeddings (BERT, etc.)\n",
        "\n",
        "# Applications to CVs:\n",
        "# - Classifying job roles (e.g., Software Engineer, Data Scientist, Marketing Manager)\n",
        "# - Classifying experience levels (e.g., Junior, Mid-level, Senior)\n",
        "# - Classifying industry\n",
        "# - Classifying required skills/qualifications\n",
        "\n",
        "# 2. Identify at least two different types of models that could be suitable for classifying CVs.\n",
        "\n",
        "# Model Type 1: Traditional ML (e.g., SVM with TF-IDF features)\n",
        "# Reasoning:\n",
        "# - Strengths: Relatively simple to understand and implement, often perform well on text classification tasks with appropriate feature engineering (like TF-IDF), computationally less expensive than deep learning models for training.\n",
        "# - Applicability to CVs: Can effectively capture the importance of specific terms (skills, job titles, degree names) using TF-IDF, which are highly relevant for CV classification. SVMs are known for their effectiveness in high-dimensional spaces like text data.\n",
        "\n",
        "# Model Type 2: Deep Learning (e.g., Transformer-based model like BERT)\n",
        "# Reasoning:\n",
        "# - Strengths: Capable of capturing complex patterns, context, and semantic relationships in text data, which is crucial for understanding the nuances in a CV. Pre-trained transformer models (like BERT) can leverage vast amounts of external text data, requiring less domain-specific data for fine-tuning.\n",
        "# - Applicability to CVs: Can understand the context in which skills, experiences, and education are presented, potentially leading to more accurate classification than models relying solely on word frequency (TF-IDF). Can handle variations in phrasing and structure.\n",
        "\n",
        "# 3. Briefly explain the reasoning behind selecting these models.\n",
        "# (Explanation is included in the reasoning section above for each model type)\n",
        "\n",
        "# 4. Do not implement or train the models at this stage, only research and select.\n",
        "# (This step is adhered to)\n",
        "\n",
        "print(\"Selected ML Models for CV Classification:\")\n",
        "print(\"- Traditional ML: Support Vector Machine (SVM) with TF-IDF features\")\n",
        "print(\"  Reasoning: Good performance on text classification, captures term importance.\")\n",
        "print(\"- Deep Learning: Transformer-based model (e.g., BERT)\")\n",
        "print(\"  Reasoning: Captures context and semantic relationships, leverages pre-training on large text corpora.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected ML Models for CV Classification:\n",
            "- Traditional ML: Support Vector Machine (SVM) with TF-IDF features\n",
            "  Reasoning: Good performance on text classification, captures term importance.\n",
            "- Deep Learning: Transformer-based model (e.g., BERT)\n",
            "  Reasoning: Captures context and semantic relationships, leverages pre-training on large text corpora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfe77d02"
      },
      "source": [
        "## Train and evaluate ml models\n",
        "\n",
        "### Subtask:\n",
        "Prepare a dataset, train the selected ML models, and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "200e5e4c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires preparing a dataset, training an SVM model with TF-IDF features, and evaluating its performance. This involves loading/creating data, preprocessing text, vectorizing using TF-IDF, splitting data, training SVM, and evaluating. I will combine these steps into a single code block. Since a dataset is not provided, I will create a synthetic one for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec93ca1f",
        "outputId": "24aa7ed5-98f3-479c-96f1-040c405f4cd9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load or create a dataset (creating a synthetic one)\n",
        "data = {\n",
        "    'cv_text': [\n",
        "        \"Experienced software engineer with skills in Python, Java, and SQL. Worked at Google for 5 years. Master's degree in Computer Science.\",\n",
        "        \"Marketing manager with expertise in digital marketing, social media, and content creation. 3 years experience at Facebook. Bachelor's degree in Marketing.\",\n",
        "        \"Data scientist with strong background in machine learning, statistics, and R. PhD in Statistics from Stanford.\",\n",
        "        \"Junior software developer with knowledge of C++ and algorithms. Internship at Microsoft. Pursuing Bachelor's degree.\",\n",
        "        \"Senior marketing specialist with focus on SEO and analytics. 10 years experience in the industry. MBA.\",\n",
        "        \"Recent graduate with a Bachelor's in Computer Science. Skills: Python, HTML, CSS. Looking for entry-level software engineering roles.\",\n",
        "        \"Experienced project manager with PMP certification. Managed large teams at Amazon. Background in engineering.\",\n",
        "        \"Graphic designer with proficiency in Adobe Creative Suite. Portfolio available. Bachelor's degree in Fine Arts.\",\n",
        "        \"Business analyst with experience in data analysis and strategic planning. Worked in consulting for 4 years. Master's in Business Administration.\",\n",
        "        \"Entry-level data analyst with skills in Excel and SQL. Internship experience. Bachelor's degree in Economics.\"\n",
        "    ],\n",
        "    'job_role': [\n",
        "        'Software Engineer', 'Marketing', 'Data Scientist', 'Software Engineer', 'Marketing',\n",
        "        'Software Engineer', 'Project Manager', 'Graphic Designer', 'Business Analyst', 'Data Analyst'\n",
        "    ],\n",
        "    'experience_level': [\n",
        "        'Senior', 'Mid-level', 'Senior', 'Junior', 'Senior',\n",
        "        'Junior', 'Senior', 'Entry-level', 'Mid-level', 'Entry-level'\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# For this subtask, we will classify by 'job_role'\n",
        "X = df['cv_text']\n",
        "y = df['job_role']\n",
        "\n",
        "# 2. Preprocess text data (tokenization, lowercase, remove stop words)\n",
        "# TF-IDF Vectorizer handles tokenization, lowercase, and can remove stop words\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "\n",
        "# 3. Convert preprocessed text data into numerical features using TF-IDF vectorization.\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
        "\n",
        "# 4. Split the dataset into training and testing sets.\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 5. Initialize and train the SVM model on the training data using the TF-IDF features.\n",
        "svm_model = SVC(kernel='linear') # Using linear kernel for simplicity and often good performance on text\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 6. Evaluate the performance of the trained SVM model on the testing data.\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"SVM Model Performance (TF-IDF Features):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Note: The dataset is very small and synthetic, so performance metrics will not be representative of real-world scenarios.\n",
        "# This serves as a demonstration of the steps."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model Performance (TF-IDF Features):\n",
            "Accuracy: 0.3333333333333333\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            " Business Analyst       0.00      0.00      0.00         1\n",
            "        Marketing       0.00      0.00      0.00         1\n",
            "Software Engineer       0.33      1.00      0.50         1\n",
            "\n",
            "         accuracy                           0.33         3\n",
            "        macro avg       0.11      0.33      0.17         3\n",
            "     weighted avg       0.11      0.33      0.17         3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e417fba6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step completed the SVM model training and evaluation. The next steps in the subtask involve working with a Deep Learning model (BERT). This requires installing necessary libraries, preprocessing data using a BERT tokenizer, preparing the data for BERT, splitting the data, loading a pre-trained BERT model, fine-tuning it, and evaluating its performance. I will combine the installation and the BERT tokenization and data preparation steps into this code block. Since we need to fine-tune BERT, we'll use the `transformers` library from Hugging Face and `torch` or `tensorflow` for the deep learning framework. I will use `torch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c79abc05",
        "outputId": "6a37b3bd-81d2-4dd3-cec6-d934a9985396"
      },
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.functional import cross_entropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd # Re-import just in case, though likely available\n",
        "\n",
        "# Assuming df, X, y are already defined from the previous step\n",
        "\n",
        "# 7. Preprocess the text data by tokenizing it using a BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Map labels to integers\n",
        "label_map = {label: i for i, label in enumerate(y.unique())}\n",
        "y_encoded = y.map(label_map)\n",
        "\n",
        "# 8. Prepare the data in a format suitable for training a BERT model\n",
        "class CVDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length', # Pad to max_len\n",
        "            truncation=True, # Truncate if longer than max_len\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt', # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define max sequence length (adjust based on typical CV length)\n",
        "MAX_LEN = 256 # Example length\n",
        "\n",
        "# 9. Split the dataset into training and testing sets.\n",
        "# Removed stratify due to small dataset size and classes with only one member\n",
        "X_train, X_test, y_train_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CVDataset(\n",
        "    texts=X_train.tolist(),\n",
        "    labels=y_train_encoded.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "\n",
        "test_dataset = CVDataset(\n",
        "    texts=X_test.tolist(),\n",
        "    labels=y_test_encoded.tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        ")\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "print(f\"Number of unique labels: {len(label_map)}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.8.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.7.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.7.9)\n",
            "Training dataset size: 7\n",
            "Testing dataset size: 3\n",
            "Number of unique labels: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82653245"
      },
      "source": [
        "**Reasoning**:\n",
        "Continue with the \"Train and evaluate ML models\" subtask. The previous step prepared the data for the BERT model. This step involves loading a pre-trained `BertForSequenceClassification` model, defining the training loop, training the model, and evaluating its performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631,
          "referenced_widgets": [
            "53e4d29deef5465d879f340271b1e3a7",
            "ff35e945eb4a4d2e8c9551592463474e",
            "4ba0567bb2a24c83913d51510583bdee",
            "81e4950c4b2d4633a9f1d911c57c4adb",
            "b5e1032f3e454c3da554c093e2c4f8ad",
            "a888c77d793c4659874bdc53d6a13219",
            "96b8a22c9db244af82e3ac3c694492f4",
            "154e46cffdc84355b1623f2e7a205be3",
            "bf142a7915df438792e9dd65a7974234",
            "c898e87762e34d8f9576d492fb800944",
            "91d2fcc708f04f6a853ad3dbc33d40d1",
            "5e1b240c2ae04c749e1c1036094f7798",
            "b50f341150364c2e9608da254a6064cb",
            "45cf9c5e0a274b2396b57a8dd6d3fd24",
            "e58ceacfa5e249b7ba0cd13bc0e85afa",
            "415acc37c5354ee1a026560b713b6bd1",
            "0c846297a12f47a787eb5e3580e741f2",
            "bbda3360890547b3bca72450f83662d4",
            "055fa879d70f4731ba61757b8ccfd677",
            "d7afcdc4b15f4457a56c88eaf36de46d",
            "55ee0682af1140fc9a52df1d39c8bf15",
            "00006645c3b0466b8d26eff9a3db6204",
            "0be2067369be4a2f943d4292f8d2e8d4",
            "f11bce8c674b4f908a3551b537ed1fb6",
            "e4d12dad53784870a5f05edc3188e64b",
            "a73a317606cc4d38964054a53f032c2a",
            "a929934214b448369ab08f37f8bc8ae3",
            "1c10d37135ca4faaa907ca7ca26c3d05",
            "c91274f1cecc494cb8aeb65a25367a69",
            "4e57e6897fc240eba90099cf5f61e02f",
            "fd348e57807f4fb698b451a0882957b7",
            "fd70de418c2d4489aa918b15186fb9d2",
            "7a83fe96ea344e9ebb86e9763c52b991",
            "b9f796c81c494991ad0b4d2724a28583",
            "17abd22fb1e54bb99aae3f9826e46051",
            "8a25760c4fa64cb7b74e6d5e092c619d",
            "8eb934ee65dd47e19632d8dfa075a377",
            "d256968143ef4ed591a6660fc602eb50",
            "e9ad059105e840e292630435b06c3ac6",
            "2617423ec8594adea597473bc62b48bb",
            "da915b2b7d3e4b3c91dc02f66a1cbdd8",
            "a8597f64cb37452d8eb4f47e4491b8eb",
            "061922d62e2a41efa1bd5b2ce92b5a6f",
            "2a57fccb28034d0d9d2ffc40f54c6e61",
            "ac7f28c790a64e108b1691971207cd45",
            "2b92587b55204a0eb78a5d642730aae2",
            "12a889f087b8481880f6a77704ae93fb",
            "8e1533f9fd7d4cd69209370cbecb5fb0",
            "6114e43c0d4b4f9e9267524cac3a18a9",
            "6b08374bd9d447f2bc4c07df44e6dd29",
            "fa31287f01d14b438689098cd4cd28dc",
            "409c71d116084cfe820c68a494504d26",
            "ae551d7e475a4640a054002a10bd0d8b",
            "e7bd32a963a245e085939f74bc387bbe",
            "fc8d28a69a7146f0af9107c0aacdf0a8"
          ]
        },
        "id": "869d4f9f",
        "outputId": "0fb8624d-5b24-42ac-9f40-d08c33ed4b29"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from tqdm.notebook import tqdm # Using tqdm.notebook for Colab progress bars\n",
        "\n",
        "# 10. Load a pre-trained BERT model for sequence classification.\n",
        "# Specify the number of output labels (number of unique job roles)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
        "\n",
        "# Set device for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 8 # Adjust based on available memory\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 11. Define the training parameters (optimizer, loss function, number of epochs).\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5) # Standard learning rate for fine-tuning BERT\n",
        "# Loss function is typically handled internally by BertForSequenceClassification,\n",
        "# but we can explicitly use CrossEntropyLoss if needed.\n",
        "\n",
        "EPOCHS = 3 # Number of training epochs (adjust as needed)\n",
        "\n",
        "# 12. Implement the training loop to fine-tune the BERT model on the training data.\n",
        "print(f\"Training BERT model on {device}...\")\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip gradients to prevent exploding gradients\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# 13. Evaluate the performance of the trained BERT model on the testing data.\n",
        "print(\"\\nEvaluating BERT model...\")\n",
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert predictions back to original labels\n",
        "id_to_label = {i: label for label, i in label_map.items()}\n",
        "predicted_labels = [id_to_label[p] for p in predictions]\n",
        "true_original_labels = [id_to_label[t] for t in true_labels]\n",
        "\n",
        "\n",
        "print(\"\\nBERT Model Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(true_original_labels, predicted_labels))\n",
        "print(\"\\nClassification Report:\")\n",
        "# Handle potential errors with classification_report on small datasets with missing labels in prediction\n",
        "try:\n",
        "    print(classification_report(true_original_labels, predicted_labels, zero_division=0))\n",
        "except ValueError as e:\n",
        "    print(f\"Could not generate detailed classification report due to missing labels in predictions: {e}\")\n",
        "    # Print a simpler report if detailed one fails\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    print(f\"Precision: {precision_score(true_original_labels, predicted_labels, average='weighted', zero_division=0):.4f}\")\n",
        "    print(f\"Recall: {recall_score(true_original_labels, predicted_labels, average='weighted', zero_division=0):.4f}\")\n",
        "    print(f\"F1-score: {f1_score(true_original_labels, predicted_labels, average='weighted', zero_division=0):.4f}\")\n",
        "\n",
        "\n",
        "# Note: Performance on this small synthetic dataset will not be indicative of real-world performance.\n",
        "# This code demonstrates the process of fine-tuning and evaluating a BERT model for text classification."
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53e4d29deef5465d879f340271b1e3a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BERT model on cpu...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e1b240c2ae04c749e1c1036094f7798"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Training Loss: 1.9778\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be2067369be4a2f943d4292f8d2e8d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average Training Loss: 1.9144\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9f796c81c494991ad0b4d2724a28583"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average Training Loss: 1.8216\n",
            "\n",
            "Evaluating BERT model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac7f28c790a64e108b1691971207cd45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Model Performance:\n",
            "Accuracy: 0.0\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            " Business Analyst       0.00      0.00      0.00       1.0\n",
            " Graphic Designer       0.00      0.00      0.00       0.0\n",
            "        Marketing       0.00      0.00      0.00       1.0\n",
            "  Project Manager       0.00      0.00      0.00       0.0\n",
            "Software Engineer       0.00      0.00      0.00       1.0\n",
            "\n",
            "         accuracy                           0.00       3.0\n",
            "        macro avg       0.00      0.00      0.00       3.0\n",
            "     weighted avg       0.00      0.00      0.00       3.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a1f5f2"
      },
      "source": [
        "## Integrate ML models\n",
        "\n",
        "### Subtask:\n",
        "Incorporate the trained ML models into the CV analysis pipeline to perform classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "299ef288"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `analyze_cv` function to include loading the trained ML models and using them to predict the job role and/or experience level based on the extracted text. Since the models trained on the synthetic data are not practical for real-world use, this implementation will focus on the structural integration, potentially using placeholder logic or assuming pre-trained models are available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bc6c3f"
      },
      "source": [
        "# Assuming the trained models (svm_model, tfidf_vectorizer for SVM and model, tokenizer for BERT)\n",
        "# are available or can be loaded here. For demonstration with synthetic data,\n",
        "# we'll use the models trained in previous steps or add placeholder logic.\n",
        "\n",
        "# It's more practical in a real application to save and load models.\n",
        "# Example of saving/loading (commented out):\n",
        "# import joblib # for SVM and TF-IDF vectorizer\n",
        "# joblib.dump(svm_model, 'svm_model.pkl')\n",
        "# joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "# svm_model_loaded = joblib.load('svm_model.pkl')\n",
        "# tfidf_vectorizer_loaded = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "# For BERT, saving/loading is done using Hugging Face methods:\n",
        "# model.save_pretrained('./bert_cv_classifier')\n",
        "# tokenizer.save_pretrained('./bert_cv_classifier')\n",
        "# bert_model_loaded = BertForSequenceClassification.from_pretrained('./bert_cv_classifier')\n",
        "# bert_tokenizer_loaded = BertTokenizer.from_pretrained('./bert_cv_classifier')\n",
        "\n",
        "\n",
        "# Placeholder function for classifying using ML models\n",
        "def classify_cv(text, svm_model, tfidf_vectorizer, bert_model, tokenizer, label_map):\n",
        "    \"\"\"Classifies the CV text using trained ML models.\"\"\"\n",
        "    # --- SVM Classification ---\n",
        "    # Preprocess and vectorize the input text\n",
        "    text_tfidf = tfidf_vectorizer.transform([text])\n",
        "    # Predict using the SVM model\n",
        "    svm_prediction_encoded = svm_model.predict(text_tfidf)[0]\n",
        "    # Convert prediction back to original label\n",
        "    id_to_label = {i: label for label, i in label_map.items()}\n",
        "    svm_predicted_label = id_to_label.get(svm_prediction_encoded, \"Unknown\")\n",
        "\n",
        "    # --- BERT Classification ---\n",
        "    # Preprocess the input text using BERT tokenizer\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN, # Use the same MAX_LEN as during training\n",
        "        return_token_type_ids=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    # Move model to evaluation mode and perform prediction\n",
        "    bert_model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = encoding['input_ids'].to(device) # Assuming 'device' is defined (cuda or cpu)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        bert_prediction_encoded = torch.argmax(logits, dim=1).flatten().item()\n",
        "\n",
        "    # Convert prediction back to original label\n",
        "    bert_predicted_label = id_to_label.get(bert_prediction_encoded, \"Unknown\")\n",
        "\n",
        "    return {\n",
        "        \"job_role_svm\": svm_predicted_label,\n",
        "        \"job_role_bert\": bert_predicted_label\n",
        "        # Could add experience level classification here too\n",
        "    }\n",
        "\n",
        "\n",
        "# Modify the analyze_cv function to include classification\n",
        "def analyze_cv(file_path, svm_model, tfidf_vectorizer, bert_model, tokenizer, label_map):\n",
        "    print(f\"Début de l'analyse du fichier: {file_path}\")\n",
        "\n",
        "    # Phase 1: Extraction\n",
        "    raw_text = extract_data(file_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    # Phase 2: Nettoyage\n",
        "    cleaned_text = clean_data(raw_text)\n",
        "\n",
        "    # Phase 3: Entités\n",
        "    entities = extract_entities(cleaned_text)\n",
        "\n",
        "    # Phase 4: Relations\n",
        "    relations = extract_relations(entities, cleaned_text)\n",
        "\n",
        "    # Phase 5: Modélisation\n",
        "    model_data = build_model(entities, relations)\n",
        "\n",
        "    # Phase 6: Classification using ML models\n",
        "    # Pass the cleaned text to the classification function\n",
        "    classification_results = classify_cv(cleaned_text, svm_model, tfidf_vectorizer, bert_model, tokenizer, label_map)\n",
        "    # Add classification results to the model data\n",
        "    model_data['candidat'].update(classification_results)\n",
        "\n",
        "\n",
        "    # Phase 7: Save to database\n",
        "    save_analysis_to_db(model_data)\n",
        "\n",
        "    # Phase 8: Présentation\n",
        "    return display_results(model_data)\n",
        "\n",
        "# Example usage (assuming models are loaded/available in the environment)\n",
        "# In a real application, you would load your trained models here before calling analyze_cv\n",
        "# For this demonstration, we'll assume the models from the previous training steps are still in memory.\n",
        "# You would also need a real CV file path here.\n",
        "# cv_path = \"chemin/vers/votre/cv.pdf\"\n",
        "# result = analyze_cv(cv_path, svm_model, tfidf_vectorizer, model, tokenizer, label_map) # Use 'model' as bert_model variable name\n",
        "\n",
        "# if result:\n",
        "#     print(\"\\n>>> Analyse terminée avec succès!\")\n",
        "#     print(f\">>> Données exportées dans 'cv_analysis.json' et sauvegardées dans '{DATABASE_NAME}'\")\n",
        "#     print(\"\\n>>> Classification Results:\")\n",
        "#     print(f\"    Job Role (SVM): {result['candidat'].get('job_role_svm', 'N/A')}\")\n",
        "#     print(f\"    Job Role (BERT): {result['candidat'].get('job_role_bert', 'N/A')}\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5463a17c"
      },
      "source": [
        "## Update database schema\n",
        "\n",
        "### Subtask:\n",
        "Modify the database schema to store the classification results from the new ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade79cd2"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the database schema to add columns for storing the classification results (job role and experience level from both SVM and BERT models). This requires connecting to the SQLite database and executing `ALTER TABLE` statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143d1b05",
        "outputId": "498273ad-9032-475e-e728-b743b8b1d93b"
      },
      "source": [
        "import sqlite3\n",
        "\n",
        "DATABASE_NAME = 'cv_analysis.db'\n",
        "\n",
        "def update_database_schema():\n",
        "    \"\"\"Adds columns for ML classification results to the candidates table.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Add columns for ML classification results if they don't exist\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN job_role_svm TEXT\")\n",
        "            print(\"Added 'job_role_svm' column to 'candidates' table.\")\n",
        "        except sqlite3.OperationalError as e:\n",
        "            if \"duplicate column name\" in str(e):\n",
        "                print(\"'job_role_svm' column already exists.\")\n",
        "            else:\n",
        "                print(f\"Error adding 'job_role_svm' column: {e}\")\n",
        "\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN job_role_bert TEXT\")\n",
        "            print(\"Added 'job_role_bert' column to 'candidates' table.\")\n",
        "        except sqlite3.OperationalError as e:\n",
        "            if \"duplicate column name\" in str(e):\n",
        "                print(\"'job_role_bert' column already exists.\")\n",
        "            else:\n",
        "                print(f\"Error adding 'job_role_bert' column: {e}\")\n",
        "\n",
        "        # Add columns for experience level classification (assuming we add that later)\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN experience_level_svm TEXT\")\n",
        "            print(\"Added 'experience_level_svm' column to 'candidates' table.\")\n",
        "        except sqlite3.OperationalError as e:\n",
        "            if \"duplicate column name\" in str(e):\n",
        "                print(\"'experience_level_svm' column already exists.\")\n",
        "            else:\n",
        "                print(f\"Error adding 'experience_level_svm' column: {e}\")\n",
        "\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN experience_level_bert TEXT\")\n",
        "            print(\"Added 'experience_level_bert' column to 'candidates' table.\")\n",
        "        except sqlite3.OperationalError as e:\n",
        "            if \"duplicate column name\" in str(e):\n",
        "                print(\"'experience_level_bert' column already exists.\")\n",
        "            else:\n",
        "                print(f\"Error adding 'experience_level_bert' column: {e}\")\n",
        "\n",
        "\n",
        "        conn.commit()\n",
        "        print(\"Database schema update process completed.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error during schema update: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Execute the function to update the database schema\n",
        "update_database_schema()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'job_role_svm' column to 'candidates' table.\n",
            "Added 'job_role_bert' column to 'candidates' table.\n",
            "Added 'experience_level_svm' column to 'candidates' table.\n",
            "Added 'experience_level_bert' column to 'candidates' table.\n",
            "Database schema update process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ae70fbb"
      },
      "source": [
        "## Refine and improve\n",
        "\n",
        "### Subtask:\n",
        "Optimize the performance of the system, improve the UI/UX of the Streamlit app, and add more sophisticated features if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e2afd0"
      },
      "source": [
        "We have reached the refinement stage. Do you have any specific aspects you'd like to improve or refine? This could include:\n",
        "\n",
        "- Improving the accuracy of the ML models (requires more data and potentially hyperparameter tuning).\n",
        "- Enhancing the UI/UX of the Streamlit application.\n",
        "- Adding more advanced entity or relation extraction rules.\n",
        "- Implementing additional classification tasks (e.g., industry).\n",
        "- Improving the database interaction or schema.\n",
        "- Adding error handling or logging.\n",
        "\n",
        "If you have specific refinements in mind, please let me know! Otherwise, we can proceed to the final step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ef13af"
      },
      "source": [
        "## Finish task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the implemented features and instructions on how to run the complete system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3251371"
      },
      "source": [
        "Once you are satisfied with the current state of the project, we can proceed to the final step, which is to summarize what we've built and provide instructions on how to run the complete CV analysis system with the Streamlit frontend, database integration, and ML classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10427390",
        "outputId": "318da7ed-0829-4aaa-e6d7-dd417600e8a3"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import sqlite3\n",
        "\n",
        "# Assuming DATABASE_NAME is defined in a previous cell\n",
        "DATABASE_NAME = 'cv_analysis.db' # Ensure DATABASE_NAME is defined\n",
        "\n",
        "# Ensure the previous cells defining the analysis functions (extract_data, clean_data,\n",
        "# extract_entities, extract_relations, build_model, display_results, analyze_cv,\n",
        "# save_analysis_to_db, create_database_and_tables, update_database_schema) have been executed.\n",
        "\n",
        "# Configuration de l'OCR (Tesseract) - needs to be set if not already\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'<CHEMIN_VERS_TESSERACT_EXE>' # Replace with your Tesseract path\n",
        "\n",
        "\n",
        "def get_analysis_from_db(candidate_name):\n",
        "    \"\"\"Queries the database for analysis results based on the candidate's name.\"\"\"\n",
        "    conn = None\n",
        "    analysis_data = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Get candidate ID and classification results\n",
        "        cursor.execute(\"SELECT id, name, contact, job_role_svm, job_role_bert, experience_level_svm, experience_level_bert FROM candidates WHERE name = ?\", (candidate_name,))\n",
        "        candidate_info = cursor.fetchone()\n",
        "\n",
        "        if candidate_info:\n",
        "            candidate_id, name, contact, job_role_svm, job_role_bert, experience_level_svm, experience_level_bert = candidate_info\n",
        "\n",
        "            analysis_data = {\n",
        "                \"candidat\": {\n",
        "                    \"nom\": name,\n",
        "                    \"contact\": contact,\n",
        "                    \"education\": [],\n",
        "                    \"competences\": [],\n",
        "                    \"experiences\": [],\n",
        "                    \"job_role_svm\": job_role_svm, # Include classification results\n",
        "                    \"job_role_bert\": job_role_bert,\n",
        "                    \"experience_level_svm\": experience_level_svm,\n",
        "                    \"experience_level_bert\": experience_level_bert\n",
        "                },\n",
        "                \"relations\": []\n",
        "            }\n",
        "\n",
        "            # Get education\n",
        "            cursor.execute(\"SELECT degree FROM education WHERE candidate_id = ?\", (candidate_id,))\n",
        "            education_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"education\"] = [row[0] for row in education_results]\n",
        "\n",
        "            # Get skills\n",
        "            cursor.execute(\"SELECT skill FROM skills WHERE candidate_id = ?\", (candidate_id,))\n",
        "            skills_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"competences\"] = [row[0] for row in skills_results]\n",
        "\n",
        "            # Get experiences\n",
        "            cursor.execute(\"SELECT organization, position, duration FROM experiences WHERE candidate_id = ?\", (candidate_id,))\n",
        "            experiences_results = cursor.fetchall()\n",
        "            analysis_data[\"candidat\"][\"experiences\"] = [{\"organisation\": row[0], \"poste\": row[1], \"duree\": row[2]} for row in experiences_results]\n",
        "\n",
        "            # Get relations\n",
        "            cursor.execute(\"SELECT relation_type, entity, context FROM relations WHERE candidate_id = ?\", (candidate_id,))\n",
        "            relations_results = cursor.fetchall()\n",
        "            analysis_data[\"relations\"] = [{\"relation\": row[0], \"entite\": row[1], \"contexte\": row[2]} for row in relations_results]\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        st.error(f\"Database error while retrieving analysis: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "    return analysis_data\n",
        "\n",
        "# Create tables and update schema if they don't exist when the app starts\n",
        "# Ensure create_database_and_tables and update_database_schema are defined elsewhere or included here\n",
        "# For simplicity, let's assume they are defined in this script for standalone execution\n",
        "def create_database_and_tables():\n",
        "    \"\"\"Connects to SQLite DB and creates tables if they don't exist.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS candidates (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                name TEXT NOT NULL,\n",
        "                contact TEXT\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS education (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                degree TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS skills (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                skill TEXT NOT NULL,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS experiences (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                organization TEXT NOT NULL,\n",
        "                position TEXT,\n",
        "                duration TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS relations (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                candidate_id INTEGER,\n",
        "                relation_type TEXT,\n",
        "                entity TEXT,\n",
        "                context TEXT,\n",
        "                FOREIGN KEY (candidate_id) REFERENCES candidates(id)\n",
        "            )\n",
        "        ''')\n",
        "        conn.commit()\n",
        "        print(f\"Database '{DATABASE_NAME}' and tables created successfully or already exist.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "def update_database_schema():\n",
        "    \"\"\"Adds columns for ML classification results to the candidates table.\"\"\"\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(DATABASE_NAME)\n",
        "        cursor = conn.cursor()\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN job_role_svm TEXT\")\n",
        "        except sqlite3.OperationalError:\n",
        "            pass # Column already exists\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN job_role_bert TEXT\")\n",
        "        except sqlite3.OperationalError:\n",
        "            pass # Column already exists\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN experience_level_svm TEXT\")\n",
        "        except sqlite3.OperationalError:\n",
        "            pass # Column already exists\n",
        "        try:\n",
        "            cursor.execute(\"ALTER TABLE candidates ADD COLUMN experience_level_bert TEXT\")\n",
        "        except sqlite3.OperationalError:\n",
        "            pass # Column already exists\n",
        "        conn.commit()\n",
        "        print(\"Database schema update process completed.\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error during schema update: {e}\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "\n",
        "# Placeholder/dummy analysis functions for standalone app.\n",
        "# In a real scenario, you'd import or define your actual analysis and ML functions here.\n",
        "def extract_data(file_path):\n",
        "    st.warning(\"Placeholder: Data extraction logic not implemented in this standalone app.\")\n",
        "    return \"Placeholder text for analysis.\" # Return dummy text for the rest of the pipeline\n",
        "\n",
        "def clean_data(text):\n",
        "     st.warning(\"Placeholder: Data cleaning logic not implemented in this standalone app.\")\n",
        "     return text # Return text as is\n",
        "\n",
        "def extract_entities(text):\n",
        "    st.warning(\"Placeholder: Entity extraction logic not implemented in this standalone app.\")\n",
        "    # Return dummy entities for demonstration\n",
        "    return {\n",
        "        \"PERSON\": [\"John Doe\"],\n",
        "        \"ORG\": [\"Example Corp\"],\n",
        "        \"DATE\": [\"2023-Present\"],\n",
        "        \"LOC\": [\"New York\"],\n",
        "        \"DIPLOME\": [\"Master's Degree\"],\n",
        "        \"COMPETENCE\": [\"Python\", \"Communication\"]\n",
        "    }\n",
        "\n",
        "def extract_relations(entities, text):\n",
        "     st.warning(\"Placeholder: Relation extraction logic not implemented in this standalone app.\")\n",
        "     # Return dummy relations\n",
        "     return [{\"relation\": \"worked_at\", \"entite\": \"Example Corp\", \"contexte\": \"Worked at Example Corp\"}]\n",
        "\n",
        "def build_model(entities, relations):\n",
        "    st.warning(\"Placeholder: Model building logic not fully implemented in this standalone app.\")\n",
        "    # Build a dummy model structure\n",
        "    return {\n",
        "        \"candidat\": {\n",
        "            \"nom\": entities[\"PERSON\"][0] if entities[\"PERSON\"] else \"Inconnu\",\n",
        "            \"contact\": \"N/A\", # Dummy contact\n",
        "            \"education\": entities[\"DIPLOME\"],\n",
        "            \"competences\": entities[\"COMPETENCE\"],\n",
        "            \"experiences\": [{\"organisation\": org, \"poste\": \"À déterminer\", \"duree\": \"\"} for org in entities[\"ORG\"]]\n",
        "        },\n",
        "        \"relations\": relations\n",
        "    }\n",
        "\n",
        "def save_analysis_to_db(model):\n",
        "    st.warning(\"Placeholder: Saving to database logic not fully implemented in this standalone app.\")\n",
        "    # In a real app, you would implement the actual database saving here\n",
        "    print(\"Placeholder: Saving model to DB:\", model)\n",
        "    pass # Dummy save\n",
        "\n",
        "# Placeholder/dummy classification function\n",
        "def classify_cv(text, svm_model=None, tfidf_vectorizer=None, bert_model=None, tokenizer=None, label_map=None):\n",
        "    st.warning(\"Placeholder: ML classification logic not implemented in this standalone app.\")\n",
        "    # Return dummy classification results\n",
        "    return {\n",
        "        \"job_role_svm\": \"Placeholder Role (SVM)\",\n",
        "        \"job_role_bert\": \"Placeholder Role (BERT)\",\n",
        "        \"experience_level_svm\": \"Placeholder Level (SVM)\",\n",
        "        \"experience_level_bert\": \"Placeholder Level (BERT)\"\n",
        "    }\n",
        "\n",
        "def analyze_cv(file_path, svm_model=None, tfidf_vectorizer=None, bert_model=None, tokenizer=None, label_map=None):\n",
        "    st.text(f\"Début de l'analyse du fichier: {os.path.basename(file_path)}\")\n",
        "\n",
        "    raw_text = extract_data(file_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    cleaned_text = clean_data(raw_text)\n",
        "    entities = extract_entities(cleaned_text)\n",
        "    relations = extract_relations(entities, cleaned_text)\n",
        "    model_data = build_model(entities, relations)\n",
        "\n",
        "    # Placeholder classification call\n",
        "    classification_results = classify_cv(cleaned_text, svm_model, tfidf_vectorizer, bert_model, tokenizer, label_map)\n",
        "    model_data['candidat'].update(classification_results)\n",
        "\n",
        "    save_analysis_to_db(model_data)\n",
        "\n",
        "    # Return the model data to be displayed by the Streamlit app\n",
        "    return model_data\n",
        "\n",
        "\n",
        "def display_results(model):\n",
        "    \"\"\"Displays the analysis results in Streamlit.\"\"\"\n",
        "    st.markdown(\"#### Informations Candidat\")\n",
        "    st.write(f\"**Nom:** {model['candidat']['nom']}\")\n",
        "    st.write(f\"**Contact:** {model['candidat']['contact']}\") # Display contact\n",
        "\n",
        "    st.markdown(\"#### Formation\")\n",
        "    if model['candidat']['education']:\n",
        "        for i, diplome in enumerate(model['candidat']['education'], 1):\n",
        "            st.write(f\"- {diplome}\")\n",
        "    else:\n",
        "        st.write(\"Aucune information de formation trouvée.\")\n",
        "\n",
        "    st.markdown(\"#### Compétences\")\n",
        "    if model['candidat']['competences']:\n",
        "        for i, competence in enumerate(model['candidat']['competences'], 1):\n",
        "             st.write(f\"- {competence}\")\n",
        "    else:\n",
        "        st.write(\"Aucune compétence trouvée.\")\n",
        "\n",
        "    st.markdown(\"#### Expériences Professionnelles\")\n",
        "    if model['candidat']['experiences']:\n",
        "        for i, exp in enumerate(model['candidat']['experiences'], 1):\n",
        "            st.write(f\"- **Organisation:** {exp.get('organisation', 'N/A')}\")\n",
        "            st.write(f\"  **Poste:** {exp.get('poste', 'À déterminer')}\")\n",
        "            st.write(f\"  **Durée:** {exp.get('duree', 'N/A')}\")\n",
        "        else:\n",
        "             st.write(\"Aucune expérience professionnelle trouvée.\")\n",
        "\n",
        "\n",
        "    st.markdown(\"#### Relations identifiées\")\n",
        "    if model['relations']:\n",
        "        for rel in model['relations']:\n",
        "             st.write(f\"- **Relation:** {rel.get('relation', 'N/A')}, **Entité:** {rel.get('entite', 'N/A')}, **Contexte:** {rel.get('contexte', 'N/A')}\")\n",
        "    else:\n",
        "        st.write(\"Aucune relation identifiée.\")\n",
        "\n",
        "    st.markdown(\"#### Classification Results\")\n",
        "    st.write(f\"**Job Role (SVM):** {model['candidat'].get('job_role_svm', 'N/A')}\")\n",
        "    st.write(f\"**Job Role (BERT):** {model['candidat'].get('job_role_bert', 'N/A')}\")\n",
        "    st.write(f\"**Experience Level (SVM):** {model['candidat'].get('experience_level_svm', 'N/A')}\")\n",
        "    st.write(f\"**Experience Level (BERT):** {model['candidat'].get('experience_level_bert', 'N/A')}\")\n",
        "\n",
        "\n",
        "# --- Streamlit App Layout ---\n",
        "\n",
        "# Create tables and update schema if they don't exist when the app starts\n",
        "create_database_and_tables()\n",
        "update_database_schema()\n",
        "\n",
        "\n",
        "st.title(\"Analyse de CV\")\n",
        "\n",
        "# Section for uploading a new CV\n",
        "st.header(\"Analyser un nouveau CV\")\n",
        "uploaded_file = st.file_uploader(\"Choisissez un fichier PDF ou image (PNG, JPG, JPEG)\", type=['pdf', 'png', 'jpg', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    st.info(f\"Fichier téléchargé: {uploaded_file.name}\")\n",
        "\n",
        "    st.text(\"Analyse en cours...\")\n",
        "    # analyze_cv now saves to DB and includes classification (assuming models are available)\n",
        "    # Note: The analyze_cv function now requires model objects.\n",
        "    # For this Streamlit app to work, you would need to load the trained models here\n",
        "    # or make them globally accessible. This is a simplification for demonstration.\n",
        "    # Assuming svm_model, tfidf_vectorizer, model (bert_model), tokenizer, label_map\n",
        "    # are available from previous cells' execution.\n",
        "    try:\n",
        "         # Pass the models and label_map to analyze_cv - using None as placeholders\n",
        "         # for this standalone app where models are not loaded.\n",
        "        analysis_result = analyze_cv(tmp_file_path, None, None, None, None, None)\n",
        "\n",
        "         # Retrieve data from the database after saving\n",
        "        if analysis_result and 'candidat' in analysis_result and 'nom' in analysis_result['candidat']:\n",
        "            candidate_name = analysis_result['candidat']['nom']\n",
        "            db_analysis_data = get_analysis_from_db(candidate_name)\n",
        "            if db_analysis_data:\n",
        "                st.success(f\"Analyse terminée et sauvegardée pour {candidate_name}!\")\n",
        "                # Display data retrieved from the database\n",
        "                st.subheader(f\"Résultats de l'analyse pour {candidate_name}\")\n",
        "                # Modify display_results to handle and show classification\n",
        "                # For now, we'll just display the classification results directly here\n",
        "                display_results(db_analysis_data) # Reuse the original display function\n",
        "\n",
        "\n",
        "            else:\n",
        "                st.warning(f\"Analyse terminée, mais les données pour {candidate_name} n'ont pas pu être récupérées de la base de données.\")\n",
        "\n",
        "        else:\n",
        "            st.error(\"Échec de l'analyse du CV. Veuillez vérifier le file content or analysis logic.\")\n",
        "\n",
        "    except NameError as e:\n",
        "        st.error(f\"Error: Required models or variables are not defined. Please ensure all previous cells, including model training, have been executed. Details: {e}\")\n",
        "    except Exception as e:\n",
        "         st.error(f\"An unexpected error occurred during analysis: {e}\")\n",
        "\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.unlink(tmp_file_path)\n",
        "\n",
        "\n",
        "# Section for viewing previously analyzed CVs\n",
        "st.header(\"Voir les analyses précédentes\")\n",
        "\n",
        "# Get list of candidates from the database\n",
        "conn = None\n",
        "candidate_names = []\n",
        "try:\n",
        "    conn = sqlite3.connect(DATABASE_NAME)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM candidates ORDER BY name\")\n",
        "    candidate_names = [row[0] for row in cursor.fetchall()]\n",
        "except sqlite3.Error as e:\n",
        "    st.error(f\"Database error while fetching candidate list: {e}\")\n",
        "finally:\n",
        "    if conn:\n",
        "        conn.close()\n",
        "\n",
        "if candidate_names:\n",
        "    selected_candidate = st.selectbox(\"Sélectionnez un candidat:\", [\"-- Sélectionner --\"] + candidate_names)\n",
        "\n",
        "    if selected_candidate != \"-- Sélectionner --\":\n",
        "        st.text(f\"Récupération de l'analyse pour {selected_candidate}...\")\n",
        "        previous_analysis_data = get_analysis_from_db(selected_candidate)\n",
        "\n",
        "        if previous_analysis_data:\n",
        "            st.subheader(f\"Résultats de l'analyse pour {selected_candidate}\")\n",
        "            display_results(previous_analysis_data) # Reuse the display function\n",
        "\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"Aucune analyse trouvée pour {selected_candidate}.\")\n",
        "else:\n",
        "    st.info(\"Aucun CV analysé n'a été trouvé dans la base de données.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dntUuDCkgGmu",
        "outputId": "f840cc61-ddc1-449b-f9fd-1802737f0413"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.74.49.169:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvX2mJ_vgM2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}